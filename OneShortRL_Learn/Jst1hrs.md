## NOTES


- Each state is associated with a value function V(s) predicting the expected amount of future rewards. 
- We are able to receive in this state by acting the corresponding policy.

```
Let’s say when we are in state s, we decide to take action a to arrive in the next state s’ and obtain reward r. 
This is known as one transition step, represented by a tuple (s, a, s’, r).

```

Model: Transition and Reward
